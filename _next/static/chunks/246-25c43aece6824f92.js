"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[246],{5191:(e,a,t)=>{t.d(a,{Mu:()=>i});let i=[{id:"wang-2025-odysseybench",title:"OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows",authors:["Weixuan Wang","Dongge Han","Daniel Madrigal Diaz","Jin Xu","Victor R\xfchle","Saravan Rajmohan"],venue:"arXiv preprint arXiv:2508.09124",venueShort:"arXiv",year:2025,date:"2025-08",abstract:"Autonomous agents powered by large language models (LLMs) are increasingly deployed in real-world applications requiring complex, long-horizon workflows. However, existing benchmarks predominantly focus on atomic tasks that are self-contained and independent, failing to capture the long-term contextual dependencies and multi-interaction coordination required in realistic scenarios. To address this gap, we introduce OdysseyBench, a comprehensive benchmark for evaluating LLM agents on long-horizon workflows across diverse office applications including Word, Excel, PDF, Email, and Calendar. Our benchmark comprises two complementary splits: OdysseyBench+ with 300 tasks derived from real-world use cases, and OdysseyBench-Neo with 302 newly synthesized complex tasks. Each task requires agent to identify essential information from long-horizon interaction histories and perform multi-step reasoning across various applications. To enable scalable benchmark creation, we propose HomerAgents, a multi-agent framework that automates the generation of long-horizon workflow benchmarks through systematic environment exploration, task generation, and dialogue synthesis. Our extensive evaluation demonstrates that OdysseyBench effectively challenges state-of-the-art LLM agents, providing more accurate assessment of their capabilities in complex, real-world contexts compared to existing atomic task benchmarks.",featured:!1,type:"arxiv",isFirstAuthor:!1,doi:"10.48550/arXiv.2508.09124",links:{pdf:"https://arxiv.org/pdf/2508.09124.pdf",arxiv:"https://arxiv.org/abs/2508.09124",doi:"https://doi.org/10.48550/arXiv.2508.09124"}},{id:"gong-2025-beyond-logits",title:"Beyond Logits: Aligning Feature Dynamics for Effective Knowledge Distillation",authors:["Guoqiang Gong","Jiaxing Wang","Jin Xu","Deping Xiang","Zicheng Zhang","Leqi Shen","Yifeng Zhang","JunhuaShu JunhuaShu","ZhaolongXing ZhaolongXing","Zhen Chen","Pengzhang Liu","Ke Zhang"],venue:"Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",venueShort:"ACL",year:2025,date:"2025-07",abstract:"Knowledge distillation (KD) compresses large language models (LLMs), known as teacher models, into lightweight versions called student models, enabling efficient inference and downstream applications. However, prevailing approaches accomplish this by predominantly focusing on matching the final output distributions of student/teacher models. Drawing on the perspective that transformers can be viewed as discretizing ordinary differential equation (ODEs) on integer time steps (corresponding to layer indices), where intermediate features evolve across layers, we argue that effective KD requires aligning the entire feature dynamics between teacher and student models, which we call feature dynamics distillation (FDD). This alignment involves matching both the feature trajectory and its first-order derivative, rather than just the final states. Our approach extends the original KD objective with two additional loss terms: layer-wise feature KD, which matches discretized feature trajectory, and layer feature delta KD, which matches first-order changes in features across adjacent layers. Extensive experiments on various tasks validate the effectiveness of our distillation method.",featured:!0,type:"conference",isFirstAuthor:!1,doi:"10.18653/v1/2025.acl-long.1125",links:{pdf:"https://aclanthology.org/2025.acl-long.1125.pdf",website:"https://aclanthology.org/2025.acl-long.1125/",doi:"https://doi.org/10.18653/v1/2025.acl-long.1125"}},{id:"han-2025-enhancing-reasoning",title:"Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search",authors:["Dongge Han","Menglin Xia","Daniel Madrigal","Samuel Kessler","Ankur Mallick","Xuchao Zhang","Mirian Del Carmen Hipolito Garcia","Jin Xu","Victor R\xfchle","Saravan Rajmohan"],venue:"ICML 2025 Workshop on Theoretical and Theoretical Foundations of Deep Learning on Graphs and Foundation Models (TTODLer-FM)",venueShort:"ICML Workshop",year:2025,date:"2025-06",abstract:"Small language models (SLMs) offer promising and efficient alternatives to large language models (LLMs). However, SLMs' limited capacity restricts their reasoning capabilities and makes them sensitive to prompt variations. To address these challenges, we propose a novel framework that enhances SLM reasoning capabilities through LLM generated blueprints. The blueprints provide structured, high-level reasoning guides that help SLMs systematically tackle related problems. Furthermore, our framework integrates a prompt template search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our framework demonstrates improved SLM performance across various tasks, including math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves the reasoning capabilities of SLMs without increasing model size or requiring additional training, offering a lightweight and deployment-friendly solution for on-device or resource-constrained environments.",featured:!1,type:"workshop",isFirstAuthor:!1,links:{pdf:"https://openreview.net/pdf?id=LsNstclw8Z",website:"https://openreview.net/forum?id=LsNstclw8Z"}},{id:"vankadara-2024-ssm-feature-learning",title:"On Feature Learning in Structured State Space Models",authors:["Leena Chennuru Vankadara*","Jin Xu*","Moritz Haas","Volkan Cevher"],venue:"NeurIPS 2024",venueShort:"NeurIPS",year:2024,date:"2024-09",abstract:"This paper studies the scaling behavior of state-space models (SSMs) and structured variants such as Mamba, focusing on their capability to learn features in the infinite-width limit. We show that common scaling rules (like Maximal Update Parameterization) fail to support feature learning in SSMs, and that spectral scaling conditions often effective elsewhere do not apply. A detailed signal propagation analysis (forward and backward) uncovers a scaling regime enabling non-trivial feature evolution in infinite-width SSMs, offering improved stability, generalization, and hyper-parameter transfer.",featured:!0,type:"conference",isFirstAuthor:!0,links:{pdf:"https://openreview.net/forum?id=aQv5AbN1wF",website:"https://openreview.net/forum?id=aQv5AbN1wF"}},{id:"haas-2024-mup2-sam",title:"μP\xb2: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling",authors:["Moritz Haas","Jin Xu","Volkan Cevher","Leena Chennuru Vankadara"],venue:"Advances in Neural Information Processing Systems",venueShort:"NeurIPS",year:2024,date:"2024-12",abstract:"Sharpness Aware Minimization (SAM) enhances performance across various neural architectures and datasets. As models are continually scaled up to improve performance, a rigorous understanding of SAM's scaling behaviour is paramount. To this end, we study the infinite-width limit of neural networks trained with SAM, using the Tensor Programs framework. Our findings reveal that the dynamics of standard SAM effectively reduce to applying SAM solely in the last layer in wide neural networks, even with optimal hyperparameters. In contrast, we identify a stable parameterization with layerwise perturbation scaling, which we call Maximal Update and Perturbation Parameterization (μP\xb2), that ensures all layers are both feature learning and effectively perturbed in the limit. Through experiments with MLPs, ResNets and Vision Transformers, we empirically demonstrate that μP\xb2 is the first parameterization to achieve hyperparameter transfer of the joint optimum of learning rate and perturbation radius across model scales.",featured:!1,type:"conference",isFirstAuthor:!1,links:{pdf:"https://proceedings.neurips.cc/paper_files/paper/2024/file/449a016a6ce6fba3fe50d05482abf836-Paper-Conference.pdf",website:"https://proceedings.neurips.cc/paper_files/paper/2024/hash/449a016a6ce6fba3fe50d05482abf836-Abstract-Conference.html"}},{id:"xu-2023-mnp",title:"Deep Stochastic Processes via Functional Markov Transition Operators",authors:["Jin Xu","Emilien Dupont","Kaspar M\xe4rtens","Tom Rainforth","Yee Whye Teh"],venue:"Advances in Neural Information Processing Systems",venueShort:"NeurIPS",year:2023,date:"2023-12",abstract:"We introduce Markov Neural Processes (MNPs), a new class of Stochastic Processes (SPs) constructed by stacking sequences of neural-parameterised Markov transition operators in function space. We prove that these operators preserve exchangeability and consistency, adding flexibility to Neural Processes without compromising consistency. Experiments demonstrate clear advantages of MNPs over baselines across tasks.",summary:"NeurIPS 2023 (Proc. Vol. 36, 2024)",featured:!0,isFirstAuthor:!0,type:"conference",doi:"10.5555/3666122.3667775",links:{arxiv:"https://arxiv.org/abs/2305.15574",pdf:"https://proceedings.neurips.cc/paper_files/paper/2023/file/7749f9c0d5ff109231be21e910a3ced2-Paper-Conference.pdf",doi:"https://doi.org/10.5555/3666122.3667775",slides:"https://docs.google.com/presentation/d/1Q5gS4ChqzMQ2IXK8uJIZg-ehoBCLfYMnFKOOKRxbp4M/edit?usp=sharing"},image:"/images/publications/xu-2023-featured.png"},{id:"xu-2021-ges",title:"Group Equivariant Subsampling",authors:["Jin Xu","Hyunjik Kim","Tom Rainforth","Yee Whye Teh"],venue:"Neural Information Processing Systems",venueShort:"NeurIPS",year:2021,date:"2021-12",abstract:"We introduce translation- and group-equivariant subsampling/upsampling layers to construct exactly equivariant CNNs and group-equivariant autoencoders. Learned representations generalize to unseen positions and orientations and show improved data efficiency and object-centric decomposition.",summary:"NeurIPS 2021",featured:!0,isFirstAuthor:!0,type:"conference",links:{arxiv:"https://arxiv.org/abs/2106.05886",pdf:"https://arxiv.org/pdf/2106.05886.pdf",code:"https://github.com/jinxu06/gsubsampling",slides:"https://docs.google.com/presentation/d/1Q5gS4ChqzMQ2IXK8uJIZg-ehoBCLfYMnFKOOKRxbp4M/edit?usp=sharing"},image:"/images/publications/xu-2021-featured.png"},{id:"xu-2020-metafun",title:"MetaFun: Meta-Learning with Iterative Functional Updates",authors:["Jin Xu","Jean-Francois Ton","Hyunjik Kim","Adam R Kosiorek","Yee Whye Teh"],venue:"International Conference on Machine Learning",venueShort:"ICML",year:2020,date:"2020-07",abstract:"We develop a functional encoder–decoder approach to supervised meta-learning, where labelled data are encoded into infinite-dimensional functional representations via learned iterative updates. The final representation conditions a decoder for predictions. Our approach is the first encoder–decoder meta-learner to achieve state-of-the-art on miniImageNet and tieredImageNet.",summary:"ICML 2020",featured:!0,isFirstAuthor:!0,type:"conference",links:{arxiv:"https://arxiv.org/abs/1912.02738",pdf:"https://arxiv.org/pdf/1912.02738.pdf",code:"https://github.com/jinxu06/metafun-tensorflow",slides:"https://docs.google.com/presentation/d/1Vju9PuzYH4nyv3tMe7sTtSjo28ut5sfl7UsyxFg1l0I/edit?usp=sharing"},image:"/images/publications/xu-2020-featured.png"},{id:"xu-2018-inpainting",title:"Controllable Probabilistic Semantic Image Inpainting",authors:["Jin Xu","Yee Whye Teh"],venue:"arXiv preprint",venueShort:"ArXiv",year:2018,date:"2018-06",abstract:"We develop a method for user-controllable semantic image inpainting using a deep generative model combining an encoder for arbitrary observed pixels, disentangled latent variables, and a bidirectional PixelCNN. Our method generates plausible, coherent inpaintings matching user-specified semantics while remaining consistent with observations.",summary:"arXiv:1806.05953",featured:!1,isFirstAuthor:!0,type:"arxiv",links:{arxiv:"https://arxiv.org/abs/1806.05953",pdf:"https://arxiv.org/pdf/1806.05953.pdf"},image:"/images/publications/xu-2018-featured.jpeg"}]},9060:(e,a,t)=>{t.d(a,{v:()=>r});var i=t(5155),n=t(2900);function r(e){let{publication:a,index:t}=e;return(0,i.jsx)(n.P.div,{initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.5,delay:.1*t},className:"bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-lg transition-shadow duration-300 p-6 border border-gray-200 dark:border-gray-700",children:(0,i.jsxs)("div",{className:"flex flex-col space-y-4",children:[(0,i.jsx)("div",{className:"flex items-center justify-between",children:(0,i.jsxs)("span",{className:"inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium ".concat("conference"===a.type?"bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200":"journal"===a.type?"bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200":"arxiv"===a.type?"bg-yellow-100 text-yellow-800 dark:bg-yellow-900 dark:text-yellow-200":"workshop"===a.type?"bg-orange-100 text-orange-800 dark:bg-orange-900 dark:text-orange-200":"bg-gray-100 text-gray-800 dark:bg-gray-700 dark:text-gray-200"),children:[a.venueShort," ",a.year]})}),(0,i.jsx)("h3",{className:"text-lg font-semibold text-gray-900 dark:text-white leading-tight",children:a.title}),(0,i.jsx)("p",{className:"text-sm text-gray-600 dark:text-gray-400",children:a.authors.map((e,t)=>(0,i.jsxs)("span",{children:[e.includes("Jin Xu")?(0,i.jsx)("strong",{className:"text-gray-900 dark:text-white",children:e}):e,t<a.authors.length-1&&", "]},t))}),(0,i.jsx)("p",{className:"text-sm font-medium text-gray-700 dark:text-gray-300",children:a.venue}),(0,i.jsx)("p",{className:"text-sm text-gray-600 dark:text-gray-400 line-clamp-3",children:a.abstract}),(0,i.jsxs)("div",{className:"flex flex-wrap gap-2",children:[a.links.pdf&&(0,i.jsx)("a",{href:a.links.pdf,target:"_blank",rel:"noopener noreferrer",className:"inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors duration-200",children:"PDF"}),a.links.code&&(0,i.jsx)("a",{href:a.links.code,target:"_blank",rel:"noopener noreferrer",className:"inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-gray-800 hover:bg-gray-900 rounded-md transition-colors duration-200",children:"Code"}),a.links.slides&&(0,i.jsx)("a",{href:a.links.slides,target:"_blank",rel:"noopener noreferrer",className:"inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-md transition-colors duration-200",children:"Slides"}),a.links.arxiv&&(0,i.jsx)("a",{href:a.links.arxiv,target:"_blank",rel:"noopener noreferrer",className:"inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-orange-600 hover:bg-orange-700 rounded-md transition-colors duration-200",children:"arXiv"}),a.links.doi&&(0,i.jsx)("a",{href:a.links.doi,target:"_blank",rel:"noopener noreferrer",className:"inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-green-600 hover:bg-green-700 rounded-md transition-colors duration-200",children:"DOI"}),a.citations&&a.citations>0&&(0,i.jsxs)("span",{className:"inline-flex items-center px-3 py-1.5 text-xs font-medium text-gray-700 dark:text-gray-300 bg-gray-100 dark:bg-gray-700 rounded-md",children:[a.citations," citations"]})]})]})})}}}]);