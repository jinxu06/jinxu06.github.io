<!DOCTYPE html><!--KKmP2DasY4jhbaToYAyUs--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/569ce4b8f30dc480-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/fe06632dce259514.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-8c94b35adf29e9b1.js"/><script src="/_next/static/chunks/4bd1b696-cf72ae8a39fa05aa.js" async=""></script><script src="/_next/static/chunks/964-38db4bd4892fef52.js" async=""></script><script src="/_next/static/chunks/main-app-0e5424266e62c1b8.js" async=""></script><script src="/_next/static/chunks/app/layout-8a8991d0d1d509dd.js" async=""></script><script src="/_next/static/chunks/900-4da7ce51ab268504.js" async=""></script><script src="/_next/static/chunks/246-25c43aece6824f92.js" async=""></script><script src="/_next/static/chunks/app/publications/page-f33e6c93501bf922.js" async=""></script><meta name="next-size-adjust" content=""/><title>Jin Xu - Ph.D. Candidate in Statistical Machine Learning</title><meta name="description" content="Personal website of Jin Xu, Ph.D. Candidate in Statistical Machine Learning at University of Oxford."/><meta name="author" content="Jin Xu"/><meta name="keywords" content="Jin Xu, machine learning, meta learning, equivariance, deep learning, Oxford, statistics"/><meta property="og:title" content="Jin Xu - Ph.D. Candidate in Statistical Machine Learning"/><meta property="og:description" content="Ph.D. Candidate in Statistical Machine Learning at University of Oxford."/><meta property="og:url" content="https://jinxu06.github.io/"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:creator" content="@jinxu06"/><meta name="twitter:title" content="Jin Xu - Ph.D. Candidate in Statistical Machine Learning"/><meta name="twitter:description" content="Ph.D. Candidate in Statistical Machine Learning at University of Oxford."/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_5cfdac __variable_9a8899 antialiased"><div hidden=""><!--$--><!--/$--></div><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","jin-xu-theme","system",null,["light","dark"],null,true,true)</script><div class="min-h-screen bg-white dark:bg-gray-900 transition-colors duration-300"><nav class="bg-white dark:bg-gray-900 shadow-sm border-b border-gray-200 dark:border-gray-700"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between h-16"><div class="flex items-center"><div class="flex-shrink-0"><h1 class="text-xl font-bold text-gray-900 dark:text-white">Jin Xu</h1></div><div class="hidden md:ml-8 md:flex md:space-x-8"><a href="/" class="text-gray-700 dark:text-gray-300 hover:text-gray-900 dark:hover:text-white px-3 py-2 text-sm font-medium transition-colors duration-200">Home</a><a href="/publications" class="text-gray-700 dark:text-gray-300 hover:text-gray-900 dark:hover:text-white px-3 py-2 text-sm font-medium transition-colors duration-200">Publications</a><a href="/posts" class="text-gray-700 dark:text-gray-300 hover:text-gray-900 dark:hover:text-white px-3 py-2 text-sm font-medium transition-colors duration-200">Posts</a><a href="/contact" class="text-gray-700 dark:text-gray-300 hover:text-gray-900 dark:hover:text-white px-3 py-2 text-sm font-medium transition-colors duration-200">Contact</a></div></div><div class="flex items-center space-x-4"><div class="p-2 rounded-md w-9 h-9"></div><div class="md:hidden"><button class="text-gray-700 dark:text-gray-300 hover:text-gray-900 dark:hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></button></div></div></div></div></nav><main class="flex-1"><div class="min-h-screen bg-white dark:bg-gray-900 py-16"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="text-center mb-16" style="opacity:0;transform:translateY(20px)"><h1 class="text-4xl sm:text-5xl font-bold text-gray-900 dark:text-white mb-4">Publications</h1></div><div class="grid grid-cols-1 lg:grid-cols-2 gap-6"><div class="bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-lg transition-shadow duration-300 p-6 border border-gray-200 dark:border-gray-700" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col space-y-4"><div class="flex items-center justify-between"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-yellow-100 text-yellow-800 dark:bg-yellow-900 dark:text-yellow-200">arXiv<!-- --> <!-- -->2025</span></div><h3 class="text-lg font-semibold text-gray-900 dark:text-white leading-tight">OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows</h3><p class="text-sm text-gray-600 dark:text-gray-400"><span>Weixuan Wang<!-- -->, </span><span>Dongge Han<!-- -->, </span><span>Daniel Madrigal Diaz<!-- -->, </span><span><strong class="text-gray-900 dark:text-white">Jin Xu</strong>, </span><span>Victor RÃ¼hle<!-- -->, </span><span>Saravan Rajmohan</span></p><p class="text-sm font-medium text-gray-700 dark:text-gray-300">arXiv preprint arXiv:2508.09124</p><p class="text-sm text-gray-600 dark:text-gray-400 line-clamp-3">Autonomous agents powered by large language models (LLMs) are increasingly deployed in real-world applications requiring complex, long-horizon workflows. However, existing benchmarks predominantly focus on atomic tasks that are self-contained and independent, failing to capture the long-term contextual dependencies and multi-interaction coordination required in realistic scenarios. To address this gap, we introduce OdysseyBench, a comprehensive benchmark for evaluating LLM agents on long-horizon workflows across diverse office applications including Word, Excel, PDF, Email, and Calendar. Our benchmark comprises two complementary splits: OdysseyBench+ with 300 tasks derived from real-world use cases, and OdysseyBench-Neo with 302 newly synthesized complex tasks. Each task requires agent to identify essential information from long-horizon interaction histories and perform multi-step reasoning across various applications. To enable scalable benchmark creation, we propose HomerAgents, a multi-agent framework that automates the generation of long-horizon workflow benchmarks through systematic environment exploration, task generation, and dialogue synthesis. Our extensive evaluation demonstrates that OdysseyBench effectively challenges state-of-the-art LLM agents, providing more accurate assessment of their capabilities in complex, real-world contexts compared to existing atomic task benchmarks.</p><div class="flex flex-wrap gap-2"><a href="https://arxiv.org/pdf/2508.09124.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors duration-200">PDF</a><a href="https://arxiv.org/abs/2508.09124" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-orange-600 hover:bg-orange-700 rounded-md transition-colors duration-200">arXiv</a><a href="https://doi.org/10.48550/arXiv.2508.09124" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-green-600 hover:bg-green-700 rounded-md transition-colors duration-200">DOI</a></div></div></div><div class="bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-lg transition-shadow duration-300 p-6 border border-gray-200 dark:border-gray-700" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col space-y-4"><div class="flex items-center justify-between"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200">ACL<!-- --> <!-- -->2025</span></div><h3 class="text-lg font-semibold text-gray-900 dark:text-white leading-tight">Beyond Logits: Aligning Feature Dynamics for Effective Knowledge Distillation</h3><p class="text-sm text-gray-600 dark:text-gray-400"><span>Guoqiang Gong<!-- -->, </span><span>Jiaxing Wang<!-- -->, </span><span><strong class="text-gray-900 dark:text-white">Jin Xu</strong>, </span><span>Deping Xiang<!-- -->, </span><span>Zicheng Zhang<!-- -->, </span><span>Leqi Shen<!-- -->, </span><span>Yifeng Zhang<!-- -->, </span><span>JunhuaShu JunhuaShu<!-- -->, </span><span>ZhaolongXing ZhaolongXing<!-- -->, </span><span>Zhen Chen<!-- -->, </span><span>Pengzhang Liu<!-- -->, </span><span>Ke Zhang</span></p><p class="text-sm font-medium text-gray-700 dark:text-gray-300">Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</p><p class="text-sm text-gray-600 dark:text-gray-400 line-clamp-3">Knowledge distillation (KD) compresses large language models (LLMs), known as teacher models, into lightweight versions called student models, enabling efficient inference and downstream applications. However, prevailing approaches accomplish this by predominantly focusing on matching the final output distributions of student/teacher models. Drawing on the perspective that transformers can be viewed as discretizing ordinary differential equation (ODEs) on integer time steps (corresponding to layer indices), where intermediate features evolve across layers, we argue that effective KD requires aligning the entire feature dynamics between teacher and student models, which we call feature dynamics distillation (FDD). This alignment involves matching both the feature trajectory and its first-order derivative, rather than just the final states. Our approach extends the original KD objective with two additional loss terms: layer-wise feature KD, which matches discretized feature trajectory, and layer feature delta KD, which matches first-order changes in features across adjacent layers. Extensive experiments on various tasks validate the effectiveness of our distillation method.</p><div class="flex flex-wrap gap-2"><a href="https://aclanthology.org/2025.acl-long.1125.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors duration-200">PDF</a><a href="https://doi.org/10.18653/v1/2025.acl-long.1125" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-green-600 hover:bg-green-700 rounded-md transition-colors duration-200">DOI</a></div></div></div><div class="bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-lg transition-shadow duration-300 p-6 border border-gray-200 dark:border-gray-700" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col space-y-4"><div class="flex items-center justify-between"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-orange-100 text-orange-800 dark:bg-orange-900 dark:text-orange-200">ICML Workshop<!-- --> <!-- -->2025</span></div><h3 class="text-lg font-semibold text-gray-900 dark:text-white leading-tight">Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search</h3><p class="text-sm text-gray-600 dark:text-gray-400"><span>Dongge Han<!-- -->, </span><span>Menglin Xia<!-- -->, </span><span>Daniel Madrigal<!-- -->, </span><span>Samuel Kessler<!-- -->, </span><span>Ankur Mallick<!-- -->, </span><span>Xuchao Zhang<!-- -->, </span><span>Mirian Del Carmen Hipolito Garcia<!-- -->, </span><span><strong class="text-gray-900 dark:text-white">Jin Xu</strong>, </span><span>Victor RÃ¼hle<!-- -->, </span><span>Saravan Rajmohan</span></p><p class="text-sm font-medium text-gray-700 dark:text-gray-300">ICML 2025 Workshop on Theoretical and Theoretical Foundations of Deep Learning on Graphs and Foundation Models (TTODLer-FM)</p><p class="text-sm text-gray-600 dark:text-gray-400 line-clamp-3">Small language models (SLMs) offer promising and efficient alternatives to large language models (LLMs). However, SLMs&#x27; limited capacity restricts their reasoning capabilities and makes them sensitive to prompt variations. To address these challenges, we propose a novel framework that enhances SLM reasoning capabilities through LLM generated blueprints. The blueprints provide structured, high-level reasoning guides that help SLMs systematically tackle related problems. Furthermore, our framework integrates a prompt template search mechanism to mitigate the SLMs&#x27; sensitivity to prompt variations. Our framework demonstrates improved SLM performance across various tasks, including math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves the reasoning capabilities of SLMs without increasing model size or requiring additional training, offering a lightweight and deployment-friendly solution for on-device or resource-constrained environments.</p><div class="flex flex-wrap gap-2"><a href="https://openreview.net/pdf?id=LsNstclw8Z" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors duration-200">PDF</a></div></div></div><div class="bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-lg transition-shadow duration-300 p-6 border border-gray-200 dark:border-gray-700" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col space-y-4"><div class="flex items-center justify-between"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200">NeurIPS<!-- --> <!-- -->2024</span></div><h3 class="text-lg font-semibold text-gray-900 dark:text-white leading-tight">On Feature Learning in Structured State Space Models</h3><p class="text-sm text-gray-600 dark:text-gray-400"><span>Leena Chennuru Vankadara*<!-- -->, </span><span><strong class="text-gray-900 dark:text-white">Jin Xu*</strong>, </span><span>Moritz Haas<!-- -->, </span><span>Volkan Cevher</span></p><p class="text-sm font-medium text-gray-700 dark:text-gray-300">NeurIPS 2024</p><p class="text-sm text-gray-600 dark:text-gray-400 line-clamp-3">This paper studies the scaling behavior of state-space models (SSMs) and structured variants such as Mamba, focusing on their capability to learn features in the infinite-width limit. We show that common scaling rules (like Maximal Update Parameterization) fail to support feature learning in SSMs, and that spectral scaling conditions often effective elsewhere do not apply. A detailed signal propagation analysis (forward and backward) uncovers a scaling regime enabling non-trivial feature evolution in infinite-width SSMs, offering improved stability, generalization, and hyper-parameter transfer.</p><div class="flex flex-wrap gap-2"><a href="https://openreview.net/forum?id=aQv5AbN1wF" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors duration-200">PDF</a></div></div></div><div class="bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-lg transition-shadow duration-300 p-6 border border-gray-200 dark:border-gray-700" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col space-y-4"><div class="flex items-center justify-between"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200">NeurIPS<!-- --> <!-- -->2024</span></div><h3 class="text-lg font-semibold text-gray-900 dark:text-white leading-tight">Î¼PÂ²: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling</h3><p class="text-sm text-gray-600 dark:text-gray-400"><span>Moritz Haas<!-- -->, </span><span><strong class="text-gray-900 dark:text-white">Jin Xu</strong>, </span><span>Volkan Cevher<!-- -->, </span><span>Leena Chennuru Vankadara</span></p><p class="text-sm font-medium text-gray-700 dark:text-gray-300">Advances in Neural Information Processing Systems</p><p class="text-sm text-gray-600 dark:text-gray-400 line-clamp-3">Sharpness Aware Minimization (SAM) enhances performance across various neural architectures and datasets. As models are continually scaled up to improve performance, a rigorous understanding of SAM&#x27;s scaling behaviour is paramount. To this end, we study the infinite-width limit of neural networks trained with SAM, using the Tensor Programs framework. Our findings reveal that the dynamics of standard SAM effectively reduce to applying SAM solely in the last layer in wide neural networks, even with optimal hyperparameters. In contrast, we identify a stable parameterization with layerwise perturbation scaling, which we call Maximal Update and Perturbation Parameterization (Î¼PÂ²), that ensures all layers are both feature learning and effectively perturbed in the limit. Through experiments with MLPs, ResNets and Vision Transformers, we empirically demonstrate that Î¼PÂ² is the first parameterization to achieve hyperparameter transfer of the joint optimum of learning rate and perturbation radius across model scales.</p><div class="flex flex-wrap gap-2"><a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/449a016a6ce6fba3fe50d05482abf836-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors duration-200">PDF</a></div></div></div><div class="bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-lg transition-shadow duration-300 p-6 border border-gray-200 dark:border-gray-700" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col space-y-4"><div class="flex items-center justify-between"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200">NeurIPS<!-- --> <!-- -->2023</span></div><h3 class="text-lg font-semibold text-gray-900 dark:text-white leading-tight">Deep Stochastic Processes via Functional Markov Transition Operators</h3><p class="text-sm text-gray-600 dark:text-gray-400"><span><strong class="text-gray-900 dark:text-white">Jin Xu</strong>, </span><span>Emilien Dupont<!-- -->, </span><span>Kaspar MÃ¤rtens<!-- -->, </span><span>Tom Rainforth<!-- -->, </span><span>Yee Whye Teh</span></p><p class="text-sm font-medium text-gray-700 dark:text-gray-300">Advances in Neural Information Processing Systems</p><p class="text-sm text-gray-600 dark:text-gray-400 line-clamp-3">We introduce Markov Neural Processes (MNPs), a new class of Stochastic Processes (SPs) constructed by stacking sequences of neural-parameterised Markov transition operators in function space. We prove that these operators preserve exchangeability and consistency, adding flexibility to Neural Processes without compromising consistency. Experiments demonstrate clear advantages of MNPs over baselines across tasks.</p><div class="flex flex-wrap gap-2"><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/7749f9c0d5ff109231be21e910a3ced2-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors duration-200">PDF</a><a href="https://docs.google.com/presentation/d/1Q5gS4ChqzMQ2IXK8uJIZg-ehoBCLfYMnFKOOKRxbp4M/edit?usp=sharing" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-md transition-colors duration-200">Slides</a><a href="https://arxiv.org/abs/2305.15574" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-orange-600 hover:bg-orange-700 rounded-md transition-colors duration-200">arXiv</a><a href="https://doi.org/10.5555/3666122.3667775" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-green-600 hover:bg-green-700 rounded-md transition-colors duration-200">DOI</a></div></div></div><div class="bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-lg transition-shadow duration-300 p-6 border border-gray-200 dark:border-gray-700" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col space-y-4"><div class="flex items-center justify-between"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200">NeurIPS<!-- --> <!-- -->2021</span></div><h3 class="text-lg font-semibold text-gray-900 dark:text-white leading-tight">Group Equivariant Subsampling</h3><p class="text-sm text-gray-600 dark:text-gray-400"><span><strong class="text-gray-900 dark:text-white">Jin Xu</strong>, </span><span>Hyunjik Kim<!-- -->, </span><span>Tom Rainforth<!-- -->, </span><span>Yee Whye Teh</span></p><p class="text-sm font-medium text-gray-700 dark:text-gray-300">Neural Information Processing Systems</p><p class="text-sm text-gray-600 dark:text-gray-400 line-clamp-3">We introduce translation- and group-equivariant subsampling/upsampling layers to construct exactly equivariant CNNs and group-equivariant autoencoders. Learned representations generalize to unseen positions and orientations and show improved data efficiency and object-centric decomposition.</p><div class="flex flex-wrap gap-2"><a href="https://arxiv.org/pdf/2106.05886.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors duration-200">PDF</a><a href="https://github.com/jinxu06/gsubsampling" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-gray-800 hover:bg-gray-900 rounded-md transition-colors duration-200">Code</a><a href="https://docs.google.com/presentation/d/1Q5gS4ChqzMQ2IXK8uJIZg-ehoBCLfYMnFKOOKRxbp4M/edit?usp=sharing" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-md transition-colors duration-200">Slides</a><a href="https://arxiv.org/abs/2106.05886" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-orange-600 hover:bg-orange-700 rounded-md transition-colors duration-200">arXiv</a></div></div></div><div class="bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-lg transition-shadow duration-300 p-6 border border-gray-200 dark:border-gray-700" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col space-y-4"><div class="flex items-center justify-between"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200">ICML<!-- --> <!-- -->2020</span></div><h3 class="text-lg font-semibold text-gray-900 dark:text-white leading-tight">MetaFun: Meta-Learning with Iterative Functional Updates</h3><p class="text-sm text-gray-600 dark:text-gray-400"><span><strong class="text-gray-900 dark:text-white">Jin Xu</strong>, </span><span>Jean-Francois Ton<!-- -->, </span><span>Hyunjik Kim<!-- -->, </span><span>Adam R Kosiorek<!-- -->, </span><span>Yee Whye Teh</span></p><p class="text-sm font-medium text-gray-700 dark:text-gray-300">International Conference on Machine Learning</p><p class="text-sm text-gray-600 dark:text-gray-400 line-clamp-3">We develop a functional encoderâdecoder approach to supervised meta-learning, where labelled data are encoded into infinite-dimensional functional representations via learned iterative updates. The final representation conditions a decoder for predictions. Our approach is the first encoderâdecoder meta-learner to achieve state-of-the-art on miniImageNet and tieredImageNet.</p><div class="flex flex-wrap gap-2"><a href="https://arxiv.org/pdf/1912.02738.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors duration-200">PDF</a><a href="https://github.com/jinxu06/metafun-tensorflow" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-gray-800 hover:bg-gray-900 rounded-md transition-colors duration-200">Code</a><a href="https://docs.google.com/presentation/d/1Vju9PuzYH4nyv3tMe7sTtSjo28ut5sfl7UsyxFg1l0I/edit?usp=sharing" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-md transition-colors duration-200">Slides</a><a href="https://arxiv.org/abs/1912.02738" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-orange-600 hover:bg-orange-700 rounded-md transition-colors duration-200">arXiv</a></div></div></div><div class="bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-lg transition-shadow duration-300 p-6 border border-gray-200 dark:border-gray-700" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col space-y-4"><div class="flex items-center justify-between"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-yellow-100 text-yellow-800 dark:bg-yellow-900 dark:text-yellow-200">ArXiv<!-- --> <!-- -->2018</span></div><h3 class="text-lg font-semibold text-gray-900 dark:text-white leading-tight">Controllable Probabilistic Semantic Image Inpainting</h3><p class="text-sm text-gray-600 dark:text-gray-400"><span><strong class="text-gray-900 dark:text-white">Jin Xu</strong>, </span><span>Yee Whye Teh</span></p><p class="text-sm font-medium text-gray-700 dark:text-gray-300">arXiv preprint</p><p class="text-sm text-gray-600 dark:text-gray-400 line-clamp-3">We develop a method for user-controllable semantic image inpainting using a deep generative model combining an encoder for arbitrary observed pixels, disentangled latent variables, and a bidirectional PixelCNN. Our method generates plausible, coherent inpaintings matching user-specified semantics while remaining consistent with observations.</p><div class="flex flex-wrap gap-2"><a href="https://arxiv.org/pdf/1806.05953.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors duration-200">PDF</a><a href="https://arxiv.org/abs/1806.05953" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1.5 text-xs font-medium text-white bg-orange-600 hover:bg-orange-700 rounded-md transition-colors duration-200">arXiv</a></div></div></div></div></div></div><!--$--><!--/$--></main><footer class="bg-white dark:bg-gray-900 border-t border-gray-200 dark:border-gray-700"><div class="max-w-7xl mx-auto py-8 px-4 sm:px-6 lg:px-8"><div class="text-center text-gray-600 dark:text-gray-400"><p>Â© 2025 Jin Xu. Built with Next.js and Tailwind CSS.</p></div></div></footer></div><script src="/_next/static/chunks/webpack-8c94b35adf29e9b1.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7280,[\"177\",\"static/chunks/app/layout-8a8991d0d1d509dd.js\"],\"ThemeProvider\"]\n3:I[1179,[\"177\",\"static/chunks/app/layout-8a8991d0d1d509dd.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[894,[],\"ClientPageRoot\"]\n7:I[8770,[\"900\",\"static/chunks/900-4da7ce51ab268504.js\",\"246\",\"static/chunks/246-25c43aece6824f92.js\",\"352\",\"static/chunks/app/publications/page-f33e6c93501bf922.js\"],\"default\"]\na:I[9665,[],\"OutletBoundary\"]\nc:I[4911,[],\"AsyncMetadataOutlet\"]\ne:I[9665,[],\"ViewportBoundary\"]\n10:I[9665,[],\"MetadataBoundary\"]\n11:\"$Sreact.suspense\"\n13:I[8393,[],\"\"]\n:HL[\"/_next/static/media/569ce4b8f30dc480-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/fe06632dce259514.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"KKmP2DasY4jhbaToYAyUs\",\"p\":\"\",\"c\":[\"\",\"publications\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"publications\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/fe06632dce259514.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_5cfdac __variable_9a8899 antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]}]}]]}],{\"children\":[\"publications\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L6\",null,{\"Component\":\"$7\",\"searchParams\":{},\"params\":{},\"promises\":[\"$@8\",\"$@9\"]}],null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",[\"$\",\"$Lc\",null,{\"promise\":\"$@d\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Le\",null,{\"children\":\"$Lf\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$L10\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$11\",null,{\"fallback\":null,\"children\":\"$L12\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$13\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"8:{}\n9:\"$0:f:0:1:2:children:2:children:1:props:children:0:props:params\"\n"])</script><script>self.__next_f.push([1,"f:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"14:I[8175,[],\"IconMark\"]\n"])</script><script>self.__next_f.push([1,"d:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Jin Xu - Ph.D. Candidate in Statistical Machine Learning\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Personal website of Jin Xu, Ph.D. Candidate in Statistical Machine Learning at University of Oxford.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Jin Xu\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Jin Xu, machine learning, meta learning, equivariance, deep learning, Oxford, statistics\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"Jin Xu - Ph.D. Candidate in Statistical Machine Learning\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"Ph.D. Candidate in Statistical Machine Learning at University of Oxford.\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://jinxu06.github.io/\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:creator\",\"content\":\"@jinxu06\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:title\",\"content\":\"Jin Xu - Ph.D. Candidate in Statistical Machine Learning\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:description\",\"content\":\"Ph.D. Candidate in Statistical Machine Learning at University of Oxford.\"}],[\"$\",\"link\",\"12\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"$L14\",\"13\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"12:\"$d:metadata\"\n"])</script></body></html>